---
title: "Primeiro download"
author: "BML"
date: "2024-05-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(rvest)
library(robotstxt)
```

## Primeiro download

Esse script baixa todos arquivos do Boletim de Produção disponíveis no site da ANP. A primeira verificação é, então, verificar se os arquivos já foram baixados. Se já foram baixados, esse script parar automaticamente. 

Se não foram baixados, continua sua execução.

```{r}
 verifica_links_antigos <- file.exists("links_antigos.csv")

{
  if (verifica_links_antigos == TRUE) {stop("The value is TRUE, so the script must end here")}
  
  print("The value is FALSE. Continue to next line in the script.")
}

```

## Verificar se podemos fazer o webscrapping

Usando a função robotstxt() não precisamos sair do R para saber isso. A resposta "TRUE" para indica que podemos prosseguir

```{r robots}
url <- "https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/boletim-mensal-da-producao-de-petroleo-e-gas-natural"

resultado_bots <- paths_allowed(
  paths  = "/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/boletim-mensal-da-producao-de-petroleo-e-gas-natural", 
  domain = "www.gov.br", 
  bot    = "*"
)



{
  if (resultado_bots == FALSE) {stop("The value is TRUE, so the script must end here")}
  
  print("The value is TRUE. Continue to next line in the script.")
}


```


## Iniciar o webscrapping

Imprimir para o console o objeto capturado na URL
```{r}
boletim_anp <- read_html(url)

boletim_anp
```


### Inspecionar a página    

Os conteúdo do site está todo dentro de <div id="content-core">.

!(img/inspecao_anp.png) 

```{r}
boletim_anp |> 
  html_elements("content-core")
```

Podemos usar o <div class="conteudo">, e em seguida reter apenas o que está à direita do elemento "a". E em seguida obter os links pelo atributo "href". E, e 

```{r}

links. <- boletim_anp |> 
  html_elements(".conteudo") |> 
  html_elements("a") |> 
  html_attr("href") 
  
```

## Inspeção do arquivo

O primeiro passo é verificar a consistência do arquivo de links. Criamos um objeto apenas com as planilhas.
```{r}
links_planilhas <- links. %>% 
  as_tibble() %>%
  filter(str_detect(value, "xlsm"))
```

Em seguida criamos uma função para inspecionar esse objeto e produzir uma tabela com o número de links por ano.
```{r}
inspecionar <- function(ano){
num_ano <- links_planilhas %>% 
  filter(str_detect(value, ano)) %>% 
  nrow() 

tibble(ano = ano,
       planilhas = num_ano)  
}
numero_planilhas <- map_df(as.character(2017:2024), inspecionar)
numero_planilhas
```
Vamos inspecionar com mais detalhes os anos que têm menos de doze planilhas por ano.

```{r}
planilhas_diferentes <- numero_planilhas %>% 
  filter(planilhas < 12) %>% 
  pull(ano)

links_planilhas %>% 
  filter(str_detect(value, planilhas_diferentes[1]) )

links_planilhas %>% 
  filter(str_detect(value, planilhas_diferentes[2]) )

links_planilhas %>% 
  filter(str_detect(value, planilhas_diferentes[3]) )


```

Aqui é uma percepção visual (não automatizada em código).

2017 está OK: apenas três arquivos porque a série começou a ser divulgada em outubro.
2020 não está ok: falta  planilha do mês 6 (de 5 vai para 7)
2024 não está OK: falta a planilha de fevereiro.

Manualmente vamos criar os links faltantes. Para um eventual erro não parar a execução do script, será usada a função "try()".

```{r}
link_quebrado1 <- "https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/arquivos-bmppgn/2020/2020-06-boletim.xlsm"

link_quebrado2 <- "https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/arquivos-bmppgn/2024/tabela-fevereiro.xlsm"

try(download.file(url = link_quebrado1, destfile = "link_quebrado1", mode = "wb"), 
    silent = TRUE)

try(download.file(url = link_quebrado2, destfile = "link_quebrado2", mode = "wb"),
  silent = TRUE
)

```
O primeiro link está válido, o segundo não. Logo, o primeiro link pode ser adicionado ao arquivo de links.

```{r}
links <- c(links., link_quebrado1)
```


### Gerar arquivo de links
Em seguida, iremos criar um arquivo ".csv",  com o conteúdo dos links de PDFs e planilhas capturados. Esse arquivo será usado no script que  atualiza os links.

```{r}
# links

write.csv2(x = links, file = "links_antigos.csv")
```


## Gerar arquivo de nomes

 
Antes de baixar os arquivos que estão nos links é preciso gerar uma lista com os nomes de todos os arquivos. 
Ex: 
https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/arquivos-bmppgn/2010/2010-12-boletim.pdf deve ser salvo em um arquivo chamado 2010-12-boletim.pdf

Isso pode ser feito através de regex.
```{r}
caminho <- "https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/boletins/arquivos-bmppgn/"
caminho2 <- "https://www.gov.br/anp/pt-br/centrais-de-conteudo/publicacoes/boletins-anp/"

nomes_arquivos <- links |> 
  str_remove_all(caminho) |>
  str_remove_all(caminho2) |> 
  str_replace_all("/", "_") |> 
  str_replace_all("2021_2021", "2021") |> 
  str_replace_all("2020_2020", "2020") |> 
  str_replace_all("2019_2019", "2019") |>
  str_replace_all("2018_2018", "2018") |>
  str_replace_all("2017_2017", "2017") |>
  str_replace_all("2016_2016", "2016") |>
  str_replace_all("2015_2015", "2015") |>
  str_replace_all("2014_2014", "2014") |>
  str_replace_all("2013_2013", "2013") |>
  str_replace_all("2012_2012", "2012") |> 
  str_replace_all("2011_2011", "2011") |> 
  str_replace_all("2010_2010", "2010") 


```

O objeto do nome dos arquivos também é usado para gerar um arquivo ".csv"
```{r}

write.csv2(x = nomes_arquivos, file = "nomes_arquivos.csv")
```


## Baixar arquivos

E agora podemos criar um link para o R baixar todos os arquivos listados em "links", dando a eles os nomes em "nomes_arquivos".

Como são muitos arquivos baixados, para não receber um "timeout" do servidor do governo é recomendável fazer o R esperar entre cada download, e recomeçar automaticamente no caso de erro.

```{r}

# criando uma função para baixar no modo binário (wb)

download_arquivos <- function(x,y){download.file(url = x, destfile =y, mode = "wb")}

# criando uma função lenta, para evitar o timeout

atraso <- rate_delay(5)
download_lento <- slowly(download_arquivos, atraso)

# tentar de novo em caso de erro
download_insistente <- insistently(download_lento, rate = rate_backoff(60))

# função map equivalente ao for loop anterior, ativar para baixar todos os arquivos
map2(links, nomes_arquivos, download_lento)

```





Com isso, todos os arquivos foram baixados localmente. Outro script será criado para baixar apenas os novos arquivos. Eles irão usar os arquivos .csv gerados aqui.

