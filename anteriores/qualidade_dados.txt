While there is not a single, agreed-to set of data quality dimensions, these formulations contain common ideas. Dimensions include some characteristics that can be measured objectively (completeness, validity, format conformity) and others that depend on heavily context or on subjective interpretation (usability, reliability, reputation). Whatever
names are used, dimensions focus on whether there is enough data (completeness), whether it is right (accuracy, validity), how well it fits together (consistency, integrity, uniqueness), whether it is up-to-date (timeliness), accessible, usable, and secure. Table 29 contains definitions of a set of data quality dimensions, about which there is general agreement
and describes approaches to measuring them.

Dimensões de qualidade:
1) Acurácia: em qual grau o dado representa entidades "da vida real". Difícil de medir, anão ser que a organização reproduza a coleta de dados e manualmente verifique a acurácia dos dados. A maior parte das medidas de acurácia depende na comparação com outra fonte de dados que se verifica acurada.

2) Completude: se refere à completude dos dados presentes. A completude pode ser verificada no nível do dataset, coluna ou registro. O data set contém todos os registros esperados? Os registros são populados corretamente? (Registros com diferentes status podem ter diferentes graus de completude esperados) Todas as colunas e atributos estão populados no grau esperado? (Algumas colunas são mandatórias, outras opcionais). Determinar a completude de um dataset com variáveis graus de completude: colunas mandatórias, colunas com valores opcionais e colunas com valores condicionais.

3) Consistência: consistência pode se referir a garantir que os dados são representados de maneira consistente em um dataset, ou em um diferente número de datasets. Consistência pode ser definida entre  um grupo de atributos  e outro atributo no mesmo registro (record-level consistency) ou entre um grupo de atributos e o mesmo atributo em diferentes registros (cross-record consistency) ou entre um tipo de atributo e o mesmo atributo em diferentes registros (consistência temporal). Consistência também pode ser usada para se referir à consistência do formato. Não confundir com acurácia ou corretude.
Características que são esperadas de serem consistentes dentro e através de um dataset podem ser usados como uma base para padronização de dados. Padronização de daods se refere ao condicionamento ou regras de entrada de dados para conteúdo e formato. Encapsular consistência como um conjunto de regras que especifica relação entre valores ou atributos, ou através de um único atributo. Por exemplo, um usuário pode esperar que o número de transações diárias não exceda 105% da média de transações dos últimos 30 dias.

4) Integridade (ou coerência) inclui idéias associadas com completudo, acurácia e consistência. Integridade geralmente se refere a integridade referencial (consistência entre objetos de dados através de uma chave de referência contida nos dois objetos) ou consistência interna, em que um dataset não tem buracos ou partes faltantes. Datasets sem integridade são vistos como corruptos, ou terem perda  de dados. Datasets sem referência podem ser chamados de "órfãos"

5) Razoabilidade - pergunta se os dados atendem as expectativas.  Por exemplo, se a distribuição de vendas por uma área geográfica "faz sentido". Sua mensuração pode ser feita de diferentes formas. Por exemplo, a razoabilidade pode ser baseada em comparação com dados de benchmark, ou com dados anteriores. Ideias sobre razoabilidade podem ser vistas como subjetivas - nesse caso, um trabalho com os usuários do dado pode revelar quais expectativas se podem esperar dos dados.

6) Atemporalidade: se refere à característica do dado.Medidas de atemporalidade podem ser feitas em termos de volatilidae - qual a frequência que o dado pode mudar, e por que motivos. Dados estáticos (ex: código telefônico de países ou estados) permanecem válidos por muito tempo, enquanto dados voláteis (preços de ações, estatísticas de desemprego) permanecem atuais por pouco tempo. Medidas de latência podem identificar a data que o dado foi criado e sua validade temporal.

7) Duplicidade/unicidade - a unicidade determina que nenhuma entidade pode existir mais de uma vez em um dataset (ex: número de CPF para cada pessoa física referenciada). Garantir a unicidade das entidades garante que o dataset se refere a uma única entidade no dataset.

8) Validade - se refere se os dados são consistentes comem um determinado domínio de valores. Um conjunto de valores pode ser determinado conjunto de valores (como uma tabela de referência), um intervalo de valores, ou uma regra que gera valores. O tipo de dado, formato e precisão devem ser considerados no domínio. O dado pode ser válido apenas para um específico tipo de tempo. A validade do dado  pode ser feita comparando com os limites do domínio. O dado pode ser válido (atender os requisitos do domínio), e ianda assim não ser acurado ou corretamente associado com registros específicos.


A figura 92 associa dimensões da qualidade de dados e conceitos associados a essas dimensões. Há sobreposição entre conceitos, e demonstra que não há um acordo ou conjunto específico.

1.3.8 Common Causes of Data Quality Issues

1.3.8.1 Issues Caused by Lack of Leadership
1.3.8.2 Issues Caused by Data Entry Processes
1.3.8.3 Issues Caused by Data Processing Functions
1.3.8.4 Issues Caused by System Design

1.3.8.5 Issues Caused by Fixing Issues
Manual data patches are changes made directly on the data in the database, not through the business rules in the
application interfaces or processing. These are scripts or manual commands generally created in a hurry and
used to ‘fix’ data in an emergency such as intentional injection of bad data, lapse in security, internal fraud, or
external source for business disruption.
Like any untested code, they have a high risk of causing further errors through unintended consequences, by
changing more data than required, or not propagating the patch to all historical data affected by the original
issue. Most such patches also change the data in place, rather than preserving the prior state and adding
corrected rows.


1.3.9 Data Profiling

Data Profiling is a form of data analysis used to inspect data and assess quality. Data profiling uses statistical
techniques to discover the true structure, content, and quality of a collection of data (Olson, 2003). A profiling
engine produces statistics that analysts can use to identify patterns in data content and structure. For example:
• Counts of nulls: Identifies nulls exist and allows for inspection of whether they are allowable or not
• Max/Min value: Identifies outliers, like negatives
• Max/Min length: Identifies outliers or invalids for fields with specific length requirements
• Frequency distribution of values for individual columns: Enables assessment of reasonability (e.g.,
distribution of country codes for transactions, inspection of frequently or infrequently occurring values,
as well as the percentage of the records populated with defaulted values)
• Data type and format: Identifies level of non-conformance to format requirements, as well as
identification of unexpected formats (e.g., number of decimals, embedded spaces, sample values)
Profiling also includes cross-column analysis, which can identify overlapping or duplicate columns and expose
embedded value dependencies. Inter-table analysis explores overlapping values sets and helps identify foreign
key relationships. Most data profiling tools allow for drilling down into the analyzed data for further
investigation.
Results from the profiling engine must be assessed by an analyst to determine whether data conforms to rules
and other requirements. A good analyst can use profiling results to confirm known relationships and uncover
hidden characteristics and patterns within and between data sets, including business rules, and validity
constraints. Profiling is usually used as part of data discovery for projects (especially data integration projects;
see Chapter 8) or to assess the current state of data that is targeted for improvement. Results of data profiling
can be used to identify opportunities to improve the quality of both data and Metadata (Olson, 2003;
Maydanchik, 2007).
While profiling is an effective way to understand data, it is just a first step to data quality improvement. It
enables organizations to identify potential problems. Solving problems requires other forms of analysis,
including business process analysis, analysis of data lineage, and deeper data analysis that can help isolate root
causes of problems.

1.3.10 Data Quality and Data Processing

Data Cleansing or Scrubbing transforms data to make it conform to data standards and domain rules. Cleansing
includes detecting and correcting data errors to bring the quality of data to an acceptable level.
It costs money and introduces risk to continuously remediate data through cleansing. Ideally, the need for data
cleansing should decrease over time, as root causes of data issues are resolved. The need for data cleansing can
be addressed by:
• Implementing controls to prevent data entry errors
• Correcting the data in the source system
• Improving the business processes that create the data
In some situations, correcting on an ongoing basis may be necessary, as re-processing the data in a midstream
system is cheaper than any other alternative.

1.3.10.2 Data Enhancement
Data enhancement or enrichment is the process of adding attributes to a data set to increase its quality and
usability. Some enhancements are gained by integrating data sets internal to an organization. External data can
also be purchased to enhance organizational data (see Chapter 10). Examples of data enhancement include:

1.3.10.3 Data Parsing and Formatting

Data Parsing is the process of analyzing data using pre-determined rules to define its content or value. Data
parsing enables the data analyst to define sets of patterns that feed into a rule engine used to distinguish between
valid and invalid data values. Matching specific pattern(s) triggers actions.
Data parsing assigns characteristics to the data values appearing in a data instance, and those characteristics
help in determining potential sources for added benefits. For example, if an attribute called ‘name’ can be
determined to have values belonging to ‘business name’ embedded within it, then the data value is identified as
the name of a business rather than the name of a person. Use the same approach for any situation in which data
values organize into semantic hierarchies such as sub-parts, parts, and assemblies.
Many data quality issues involve situations where variation in data values representing similar concepts
introduces ambiguity. Extract and rearrange the separate components (commonly referred to as ‘tokens’) can be
extracted and rearranged into a standard representation to create a valid pattern. When an invalid pattern is
recognized, the application may attempt to transform the invalid value into one that meets the rules. Perform
standardization by mapping data from some source pattern into a corresponding target representation.
For example, consider the different ways telephone numbers expected to conform to a numbering plan are
formatted. While some have digits, some have alphabetic characters, and all use different special characters for
separation. People can recognize each one as a telephone number. However, to determine if these numbers are
accurate (perhaps by comparing them to a master customer directory), or to investigate whether duplicate
numbers exist when there should be only one for each supplier, the values must be parsed into their component
segments (area code, exchange, and line number) and then transformed into a standard format.
Another good example is a customer name, since names may be represented in thousands of different forms. A
good standardization tool will be able to parse the different components of a customer name, such as given
name, middle name, family name, initials, titles, generational designations, and then rearrange those
components into a canonical representation that other data services will be able to manipulate.
The human ability to recognize familiar patterns contributes to an ability to characterize variant data values
belonging to the same abstract class of values; people recognize different types of telephone numbers because
they conform to frequently used patterns. An analyst describes the format patterns that all represent a data
object, such as Person Name, Product Description, and so on. A data quality tool parses data values that
conform to any of those patterns, and even transforms them into a single, standardized form that will simplify
the assessment, similarity analysis, and remediation processes. Pattern-based parsing can automate the
recognition and subsequent standardization of meaningful value components.

1.3.10.4 Data Transformation and Standardization
During normal processing, data rules trigger and transform the data into a format that is readable by the target
architecture. However, readable does not always mean acceptable. Rules are created directly within a data
integration stream, or rely on alternate technologies embedded in or accessible from within a tool.
Data transformation builds on these types of standardization techniques. Guide rule-based transformations by
mapping data values in their original formats and patterns into a target representation. Parsed components of a
pattern are subjected to rearrangement, corrections, or any changes as directed by the rules in the knowledge
base. In fact, standardization is a special case of transformation, employing rules that capture context,
linguistics, and idioms recognized as common over time, through repeated analysis by the rules analyst or tool
vendor. (See Chapter 3.)

2. Activities
2.1 Define High Quality Data
2.2 Define a Data Quality Strategy
2.3 Identify Critical Data and Business Rules
2.4 Perform an Initial Data Quality Assessment
2.5 Identify and Prioritize Potential Improvements
2.6 Define Goals for Data Quality Improvement
2.7 Develop and Deploy Data Quality Operations

Tabela com exemplos
Formas de remediar

Para uso contínuo:
Definir:
- Ferramentas de análise e extração de dados
- Definir regras para correção de dados de baixa qualidade
- Definir repositórios para metadados
- definir regras preventivas 
- definir métricas para qualidade de dados
- realizar controle estatístico de qualidade de dados
- realizar análises de "root cause" do surgimento de dados de baixa qualidade

Governnaça e qualidade de dados
- definir política de qualidade de dados
- definir métricas para qualidade de dados


